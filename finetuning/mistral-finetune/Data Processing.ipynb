{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pmui/anaconda3/envs/rag/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "tokenizer_config.json: 100%|██████████| 1.46k/1.46k [00:00<00:00, 3.52MB/s]\n",
      "tokenizer.model: 100%|██████████| 493k/493k [00:01<00:00, 453kB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:02<00:00, 776kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 438/438 [00:00<00:00, 884kB/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\"\n",
    ")\n",
    "\n",
    "BASE_FILENAMES = [\n",
    "    f\"data/{{}}-1.txt\",\n",
    "    f\"data/{{}}-2.txt\",\n",
    "    f\"data/{{}}-3.txt\",\n",
    "    f\"data/{{}}-4.txt\",\n",
    "    f\"data/{{}}-5.txt\",\n",
    "]\n",
    "\n",
    "INSTRUCTION = \"Summarize the following story in my style\"\n",
    "INSTRUCTIONS_FILENAME = \"instructions.jsonl\"\n",
    "STORIES_FILENAME = \"stories.jsonl\"\n",
    "SUMMARIES_FILENAME = \"summaries.jsonl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_filename = f\"data/{{}}-1.txt\"\n",
    "\n",
    "# Read the story and the summary files\n",
    "with open(base_filename.format(\"story\"), 'r') as file:\n",
    "    story = \"\".join(file.readlines())\n",
    "with open(base_filename.format(\"summary\"), \"r\") as file:\n",
    "    summary = \"\".join(file.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count tokens\n",
    "instruction_tokens = tokenizer(INSTRUCTION, return_tensors=\"pt\")[\"input_ids\"].shape[1]\n",
    "story_tokens = tokenizer(story, return_tensors=\"pt\")[\"input_ids\"].shape[1]\n",
    "summary_tokens = tokenizer(summary, return_tensors=\"pt\")[\"input_ids\"].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total          Instruction    Story          Summary        \n",
      "12429          10             12164          255            \n"
     ]
    }
   ],
   "source": [
    "print(f\"{'Total':<15}{'Instruction':<15}{'Story':<15}{'Summary':<15}\")\n",
    "\n",
    "# Print table of tokens\n",
    "total_tokens = instruction_tokens + story_tokens + summary_tokens\n",
    "print(f\"{total_tokens:<15}{instruction_tokens:<15}{story_tokens:<15}{summary_tokens:<15}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the training files\n",
    "with open(INSTRUCTIONS_FILENAME, 'a') as file:\n",
    "    file.write(json.dumps({\"text\": INSTRUCTION}) + \"\\n\")\n",
    "with open(STORIES_FILENAME, 'a') as file:\n",
    "    file.write(json.dumps({\"text\": story}) + \"\\n\")\n",
    "with open(SUMMARIES_FILENAME, 'a') as file:\n",
    "    file.write(json.dumps({\"text\": summary}) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration across all 6 stories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total       Instruction Story       Summary     \n",
      "12429       10          12164       255         \n",
      "13843       10          13510       323         \n",
      "4150        10          3872        268         \n",
      "9569        10          9200        359         \n",
      "18380       10          18054       316         \n"
     ]
    }
   ],
   "source": [
    "print(f\"{'Total':<12}{'Instruction':<12}{'Story':<12}{'Summary':<12}\")\n",
    "for base_filename in BASE_FILENAMES:\n",
    "    # Read the story and the summary files\n",
    "    with open(base_filename.format(\"story\"), 'r') as file:\n",
    "        story = \"\".join(file.readlines())\n",
    "    with open(base_filename.format(\"summary\"), \"r\") as file:\n",
    "        summary = \"\".join(file.readlines())\n",
    "    \n",
    "    # Count tokens\n",
    "    instruction_tokens = tokenizer(INSTRUCTION, return_tensors=\"pt\")[\"input_ids\"].shape[1]\n",
    "    story_tokens = tokenizer(story, return_tensors=\"pt\")[\"input_ids\"].shape[1]\n",
    "    summary_tokens = tokenizer(summary, return_tensors=\"pt\")[\"input_ids\"].shape[1]\n",
    "    # Print table of tokens\n",
    "    total_tokens = instruction_tokens + story_tokens + summary_tokens\n",
    "    print(f\"{total_tokens:<12}{instruction_tokens:<12}{story_tokens:<12}{summary_tokens:<12}\")\n",
    "\n",
    "    # Write the training files\n",
    "    with open(INSTRUCTIONS_FILENAME, 'a') as file:\n",
    "        file.write(json.dumps({\"text\": INSTRUCTION}) + \"\\n\")\n",
    "    with open(STORIES_FILENAME, 'a') as file:\n",
    "        file.write(json.dumps({\"text\": story}) + \"\\n\")\n",
    "    with open(SUMMARIES_FILENAME, 'a') as file:\n",
    "        file.write(json.dumps({\"text\": summary}) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
